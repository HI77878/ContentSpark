# üîß ANALYZER FIX MASTERPLAN - TikTok Video Analysis System

**Erstellt**: 2025-07-08  
**Ziel**: Alle 22 Analyzer liefern perfekte Daten f√ºr 100% Video-Rekonstruktion bei <3x Realtime Performance

## üìä AKTUELLE PROBLEME

### 1. **QWEN2-VL TEMPORAL ANALYZER - KRITISCHER BUG** ‚ùå
- **Problem**: Nach 23 Sekunden wiederholt das Model nur noch "possibly to pick something up" 
- **Impact**: 45.6% der Daten sind unbrauchbar bei Videos >23s
- **Ursache**: Context Window Overflow / Attention Mechanism Collapse

### 2. **DATENSTRUKTUR CHAOS** ‚ö†Ô∏è
- **Problem**: Inkonsistente Feldnamen zwischen Analyzern
  - Eye Tracking: `gaze_direction_general` vs erwartet `gaze_direction`
  - Speech Rate: `average_pitch` vs erwartet `pitch_hz`
  - Object Detection: Daten direkt im Segment vs `objects` Array
- **Impact**: Datenextraktion schl√§gt fehl, obwohl Daten vorhanden sind

### 3. **PERFORMANCE BOTTLENECK** üêå
- **Problem**: 4.5x Realtime statt Ziel <3x
- **Ursachen**:
  - Keine TensorRT Optimierung f√ºr YOLOv8/Whisper
  - Suboptimale Batch Sizes
  - GPU Memory Fragmentation

## üöÄ L√ñSUNGSANS√ÑTZE (BASIEREND AUF RESEARCH)

### 1. QWEN2-VL REPETITION BUG FIX

#### A. **Sliding Window Chunking** (Empfohlen)
```python
class ChunkedVideoProcessor:
    def __init__(self):
        self.chunk_duration = 20.0  # Sekunden
        self.overlap_duration = 5.0  # Sekunden Overlap
        self.max_context_length = 15  # Letzte 15 Beschreibungen als Context
```

**Vorteile**:
- Verhindert Context Overflow
- Erh√§lt temporale Koh√§renz durch Overlap
- Skaliert auf beliebig lange Videos

#### B. **Dynamic Frame Sampling**
```python
def get_adaptive_frame_interval(video_duration, current_position):
    if current_position < 30:
        return 1.0  # 1 FPS f√ºr erste 30s
    elif current_position < 60:
        return 2.0  # 0.5 FPS f√ºr 30-60s
    else:
        return 3.0  # 0.33 FPS f√ºr >60s
```

#### C. **Context Window Reset**
```python
def reset_context_if_repetitive(descriptions):
    # Erkenne Wiederholungen
    last_5 = descriptions[-5:]
    if len(set(last_5)) == 1:  # Alle gleich
        self.scene_context = None
        self.previous_descriptions = []
```

### 2. DATA NORMALIZATION LAYER

#### Implementierung eines einheitlichen Output Schemas:

```python
class AnalyzerOutputNormalizer:
    """Standardisiert alle Analyzer Outputs"""
    
    FIELD_MAPPINGS = {
        'eye_tracking': {
            'gaze_direction_general': 'gaze_direction',
            'blickrichtung': 'gaze_direction',
            'augen_zustand': 'eye_state'
        },
        'speech_rate': {
            'average_pitch': 'pitch_hz',
            'pitch_range': 'pitch_range_hz'
        },
        'object_detection': {
            'object': 'object_class',
            'class': 'object_class',
            'label': 'object_class'
        }
    }
    
    def normalize(self, analyzer_name: str, data: dict) -> dict:
        """Normalisiert Analyzer Output"""
        if analyzer_name in self.FIELD_MAPPINGS:
            mapping = self.FIELD_MAPPINGS[analyzer_name]
            normalized = {}
            
            for key, value in data.items():
                # Map field if needed
                new_key = mapping.get(key, key)
                normalized[new_key] = value
                
            # Add original key as fallback
            for old_key, new_key in mapping.items():
                if old_key in data and new_key not in normalized:
                    normalized[new_key] = data[old_key]
                    
        return normalized
```

### 3. PERFORMANCE OPTIMIERUNGEN

#### A. **TensorRT f√ºr YOLOv8** (5x Speedup)
```bash
# Export YOLOv8 to TensorRT
yolo export model=yolov8x.pt format=engine half=True device=0 batch=16

# In Python:
from ultralytics import YOLO
model = YOLO('yolov8x.engine')  # L√§dt TensorRT optimiertes Model
```

#### B. **Faster-Whisper Integration** (4x Speedup)
```python
# Statt OpenAI Whisper:
from faster_whisper import WhisperModel

model = WhisperModel("large-v3", 
                     device="cuda", 
                     compute_type="float16",
                     num_workers=4)  # Parallel processing
```

#### C. **GPU Memory Pre-Allocation**
```python
# Verhindert Fragmentation
torch.cuda.empty_cache()
torch.cuda.set_per_process_memory_fraction(0.9)
torch.backends.cudnn.benchmark = True  # Auto-tune kernels
```

## üìù IMPLEMENTIERUNGS-ROADMAP

### PHASE 1: QWEN2-VL FIX (Priorit√§t: KRITISCH)
**Zeitsch√§tzung**: 2-3 Stunden

1. **Backup aktueller Analyzer**
   ```bash
   cp analyzers/qwen2_vl_temporal_analyzer.py analyzers/qwen2_vl_temporal_analyzer.py.backup
   ```

2. **Implementiere Chunked Processing**
   - Modify `analyze()` method f√ºr chunk-basierte Verarbeitung
   - Add `process_video_chunks()` method
   - Implement `merge_chunk_results()` f√ºr nahtlose √úberg√§nge

3. **Context Management**
   - Limit `self.previous_descriptions` auf 15 Eintr√§ge
   - Reset bei Repetition Detection
   - Pass context zwischen chunks

4. **Validation**
   - Test mit Chase Ridgeway Video (68.4s)
   - Verify keine Wiederholungen nach 23s
   - Check temporale Koh√§renz

### PHASE 2: DATA NORMALIZATION (Priorit√§t: HOCH)
**Zeitsch√§tzung**: 1-2 Stunden

1. **Create Normalizer Class**
   - File: `utils/output_normalizer.py`
   - Implement field mappings f√ºr alle 22 Analyzer
   - Add unit tests

2. **Integration in API**
   - Modify `stable_production_api_multiprocess.py`
   - Add normalization step nach analyzer execution
   - Log transformations f√ºr debugging

3. **Validation**
   - Test eye_tracking data extraction
   - Verify pitch data availability
   - Check backward compatibility

### PHASE 3: PERFORMANCE BOOST (Priorit√§t: MITTEL)
**Zeitsch√§tzung**: 3-4 Stunden

1. **TensorRT Optimizations**
   ```python
   # analyzers/object_detection_tensorrt.py
   class GPUBatchObjectDetectionTRT(GPUBatchAnalyzer):
       def _load_model_impl(self):
           self.model = YOLO('weights/yolov8x.engine')
   ```

2. **Faster-Whisper Migration**
   ```python
   # analyzers/speech_transcription_faster.py
   self.model = WhisperModel("large-v3", 
                            device="cuda",
                            compute_type="float16")
   ```

3. **GPU Groups Rebalancing**
   - Update `configs/gpu_groups_config.py`
   - Increase batch sizes f√ºr TensorRT models
   - Optimize stage concurrency

### PHASE 4: TESTING & VALIDATION (Priorit√§t: HOCH)
**Zeitsch√§tzung**: 1 Stunde

1. **Performance Benchmarks**
   ```bash
   python3 benchmark_analyzers.py --video chase_ridgeway.mp4
   ```

2. **Data Quality Checks**
   - No repetitions in Qwen2-VL
   - All fields properly mapped
   - 100% reconstruction score

3. **System Integration Test**
   ```bash
   curl -X POST http://localhost:8003/analyze \
     -d '{"video_path": "/path/to/test.mp4"}'
   ```

## üéØ ERWARTETE ERGEBNISSE

### Nach Implementierung:

1. **Qwen2-VL**: 
   - ‚úÖ 100% koh√§rente Beschreibungen f√ºr gesamte Videol√§nge
   - ‚úÖ Keine Wiederholungen nach 23s
   - ‚úÖ Detaillierte Action-Beschreibungen

2. **Data Extraction**:
   - ‚úÖ Eye Tracking: `gaze_direction` konsistent verf√ºgbar
   - ‚úÖ Voice Pitch: `pitch_hz` in allen Segmenten
   - ‚úÖ Einheitliches Schema f√ºr alle Analyzer

3. **Performance**:
   - ‚úÖ 2.5-2.8x Realtime (Ziel: <3x erreicht)
   - ‚úÖ GPU Utilization: 85-95%
   - ‚úÖ Stabile Memory Usage ohne OOM

4. **System**:
   - ‚úÖ Alle 22 Analyzer funktionsf√§hig
   - ‚úÖ 100% Reconstruction Score
   - ‚úÖ Production-ready f√ºr alle Video-L√§ngen

## üîç MONITORING & VALIDATION

### Metriken zum Tracken:
```python
metrics = {
    'qwen2_repetition_rate': 0.0,  # Ziel: 0%
    'data_extraction_failures': 0,   # Ziel: 0
    'average_realtime_factor': 2.5,  # Ziel: <3.0
    'gpu_memory_peak': 0.85,         # Ziel: <0.90
    'reconstruction_score': 100.0    # Ziel: 100%
}
```

## üìö REFERENZEN

- [Qwen2.5-VL Blog](https://qwenlm.github.io/blog/qwen2.5-vl/) - Context Window bis 1h+ Video
- [TensorRT YOLOv8 Guide](https://docs.ultralytics.com/integrations/tensorrt/) - 5x Speedup
- [Faster-Whisper](https://github.com/SYSTRAN/faster-whisper) - 4x schneller als OpenAI
- [Video-LLM Chunking Best Practices](https://www.pinecone.io/learn/chunking-strategies/)

---

**N√§chster Schritt**: Beginne mit Phase 1 - Qwen2-VL Chunking Implementation